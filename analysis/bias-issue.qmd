---
title: "Bias issue - empirical latent vs truth"
format: html
---

```{r}


library(here)
library(purrr)
library(survival)
library(tidyverse)
library(magrittr)
library(huxtable)
library(ggplot2)
purrr::walk(.x = here("R", dir(here("R"))),
            .f = source)
```


## Introduction

In Kehl (2023), (doi: 10.1158/1055-9965.EPI-22-0875), they generate time to event cohorts under various conditions.  The focus of the paper is on the bias in median survival, comparing over statistical methods and data generation schemes.

Let T be the time to event of interest, and X be the time to cohort entry, and C be the time to censoring. If I'm understanding the paper code, the "true" median survival is calculated based on the latent survival times before the truncation and censoring are applied.  That is, all T.  The motivation for this is that the true distribution (and therefore median) T probably does not have a closed form solution with the complicated data generation schemes required by the scientific setting. 

I believe this will cause a systematic underestimation of the true bias, because many of the latent T are also observed, so deviations from the underlying truth would tend to correlate in the observed and latent observations.

The point of this document is to investigate that suspicion empirically using simulations.  If there is an issue with this, there are proposals to alter the simulation scheme.

## Data generation

We want a clear underlying truth, so our data generation methods are simple.  We generate $T, X, C$ (the event, cohort entry and censoring times respectively) from uncorrelated exponential distributions.  Setting a goal of 20% truncation and 20% censoring, we set

- $\lambda_t := 1$
- $\lambda_x := 4$
- $\lambda_c := 0.27$

The following replication show that these rates give us roughly the correct proportions for censoring and truncation:

```{r}
reps <- purrr::map_dfr(
    .x = sample.int(n = 100),
    .f = (function(x) {
        gen_one_arm_exp(lambda_t = 1, 
                        lambda_x = 4, 
                        lambda_c = 0.27, 
                        n = 1000,
                        gen_seed = x) %>%
            eval_prop_trunc_cens(.)
    })
)
reps %>% summarize(across(.cols = everything(), .fns = ~mean(.)))
```

Our statistical methods only have access to $Y := \min(T,C)$ and the marker for an event, $E := I(T<C)$.  The true median survival time under these settings is $\ln(2)/\lambda_t = 0.6931.

## Two concepts of truth

We will compare the two concepts of truth:

- **Analytic:** The analytically known median for the data generating process:  $\ln(2)/\lambda_t = 0.6931$.
- **Empirical:** The mean $T$ in the underlying latent distribution, ignoring truncation and censoring.

These will be used within each simulation to find the two bias calculations (analytical and empirical), and then we can average over all simulations to find whether the two concepts differ.

## Simulation

Expand the chunk below to see the simulation code.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
reps <- 500
sims <- tibble(
    n = c(20, 50, 500, 5000),
    lambda_t = 1,
    lambda_c = 0.27,
    lambda_x = 4
) %>%
    slice(rep(1:n(), each = reps)) %>%
    mutate(repetition = rep(1:reps, times = n()/reps),
           rand_seed = sample.int(n()))

sims %<>%
    mutate(
        res = purrr::pmap(
            .l = list(lambda_t = lambda_t,
                      lambda_x = lambda_x,
                      lambda_c = lambda_c,
                      n = n, 
                      gen_seed = rand_seed),
            .f = gen_one_arm_exp
        )
    ) 

sims %<>% 
    mutate(
        observed = purrr::map(.x = res, .f = \(x) x$observed),
        latent = purrr::map(.x = res, .f = \(x) x$latent),
    )

# Get the "latent empirical" and analytical truth in each simulation:
sims %<>%
    mutate(
        median_surv_true_empirical = purrr::map_dbl(
            .x = latent,
            .f = \(x) median(x$t, na.rm = T)
        ),
        median_surv_true_analytical = log(2)/lambda_t
    )

# Estimate the median survival in each simulation using a reasonable method:
sims %<>%
    mutate(
        ev_risk_set = purrr::map_dfr(
            .x = observed,
            .f = method_risk_set_adj_one_arm
        )
    ) %>%
    unnest(ev_risk_set) 

sims %<>%
    mutate(
        bias_analytical = median_surv_hat - median_surv_true_analytical,
        bias_empirical = median_surv_hat - median_surv_true_empirical,
        coverage_analytical = coverage_helper(
            median_surv_true_analytical,
            median_surv_hat_lcb,
            median_surv_hat_ucb
        ),
        coverage_empirical = coverage_helper(
            median_surv_true_empirical,
            median_surv_hat_lcb,
            median_surv_hat_ucb
        )
    )

sim_long <- sims %>%
    select(n, contains("bias_"), contains("coverage_")) %>%
    pivot_longer(
        cols = -n,
        names_to = "measure",
        values_to = "val"
    ) %>%
    tidyr::separate(col = "measure", into = c("metric", "type")) %>%
    mutate(abs_val = abs(val)) %>%
    # the previous did nothing to coverage, but we mark it as missing anwyay
    # because it makes no sense.
    mutate(abs_val = if_else(metric %in% "coverage",
                             NA_real_,
                             abs_val))

sim_sum <- sim_long %>% 
    group_by(metric, n, type) %>%
    summarize(
        mean = mean(val, na.rm = T),
        mad = median(abs_val, na.rm = T),
        .groups = "drop"
    ) 
```

## Results

Average bias, median absolute deviation of bias, and the coverage proportion for each simulation setting:

```{r}
sim_sum %>%
    huxtable::huxtable(.) %>%
    huxtable::theme_compact(.)
```

Plot of the outcomes from each simulation:

```{r}

sim_plot <- filter(sim_long, metric %in% "bias") %>%
    mutate(n = factor(n))
ggplot(
    data = sim_plot,
    aes(x = n, y = val, fill = type)
) +
    geom_boxplot(outlier.shape = NA) + 
    geom_point(position = position_jitterdodge(
        jitter.width= 0.2
    ),
               alpha = 0.1,
               size = 0.25,
               color = "black") + 
    #     dodge.width = 0.5, 
    #     jitter.width = 0.1, 
    #     jitter.height = 0),
    #     alpha = 0.5,
    #     size = 0.25
    # ) + 
    labs(x = "Sample size", y = "Per simluation bias")
    

```


## Conclusion

The issue is there - at small sample sizes the bias magnitude (MAD) in each simulation is systematically underestimated.  However, the effect seems relatively small, and would not account for any major problems with their large sample size conclusions.  There is a larger effect on coverage (this concept of truth overestimates coverage) that does *not* appear to disappear with sample size.

We can fix this in the future by keeping the complicated data generation processes, but use a fixed grid of parameters for things that are typically set in advance for the study.  For example, fix sample size, then generate the data that way millions of times to get a less noisy idea of the truth (which we presume cannot be calculated in a closed form way).  We could also use a grid of parameters for distribution parameters, hazard rate, proportion of the cohort with a biomarker, etc. 

It's hard to know how severe this problem would be in more complicated data generation scenarios, but we can investigate that further once we reconfigure the code to operate on more fixed parameters.
